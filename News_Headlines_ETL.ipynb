{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a40b08",
   "metadata": {},
   "source": [
    "# Implementing an ETL Workflow Using Pandas for Web Scraping: Top-5 News Headlines and Sentiment Analysis with VADER NLP Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2351a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all required libraries and modules\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "# news_scraping() function will send get request to extraxt all html tags on the weblink page for top-5 news Headlines\n",
    "# returns news as text format\n",
    "\n",
    "def news_scraping():\n",
    "    r=requests.get(r'https://www.firstpost.com/category/india')\n",
    "    news=r.text\n",
    "    return news\n",
    "\n",
    "\n",
    "#data=news_scraping()  check html page extracted from above function.\n",
    "\n",
    "#beautifulsoup_extract_element() calls news_scraping function\n",
    "#creates soup and find all particular tags for top-5 news under css class story-list\n",
    "\n",
    "def beautifulsoup_extract_element():\n",
    "    html_raw=news_scraping()\n",
    "    soup=bs(html_raw,'html.parser')\n",
    "    result=soup.find_all('ul',class_=\"story-list-ul\")\n",
    "    result=result[0]\n",
    "    result=result.find_all('a')\n",
    "# We will extract needful information under story list,with below defined 3 list data structures.\n",
    "    \n",
    "    title=[]   #tile:news headline\n",
    "    desc=[]    # Short summary of the news\n",
    "    web_link=[] #Webink to access the news article page\n",
    "    for i in range(len(result)):\n",
    "        title.append(result[i].h3.text)\n",
    "        desc.append(result[i].h4.text)\n",
    "        web_link.append(result[i].get('href'))\n",
    "\n",
    "    title=[item.strip() for item in title]  #cleaning spaces\n",
    "    desc=[item.strip() for item in desc]\n",
    "    #runtime timestamp for next update\n",
    "    runtime=datetime.now().strftime(\"%x %H:%M\") #formated with Date and Hr,min for every runtime\n",
    "        \n",
    "    newsframe_dic={\"title\":title,\"short_summ\":desc,\"News_Link\":web_link,\"News_time\":runtime}  #dict with all above lists\n",
    "    df=pd.DataFrame(newsframe_dic)   #converting to dataframe with dict defined colmun names.\n",
    "    return df\n",
    "\n",
    "# For Sentiment analysis, used Vandor NLP Model to understand the mood genere of news article.\n",
    "# Sentiment Polarity is applied on Short summry of every 5 news articles with every runtime\n",
    "# Based on compound result, A condition is implemented to tag news as Neutral,Positive or Negative.\n",
    "\n",
    "def sentiment_analyzer():\n",
    "    f=beautifulsoup_extract_element()\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    ids=[]\n",
    "    neg=[]\n",
    "    pos=[]\n",
    "    neu=[]\n",
    "    compound=[]\n",
    "    for i,row in (f.iterrows()):\n",
    "        #print(i,row['Short_sum'])\n",
    "        score_dict=sia.polarity_scores(row['short_summ'])\n",
    "        ids.append(i)\n",
    "        neg.append(score_dict['neg'])\n",
    "        pos.append(score_dict['pos'])\n",
    "        neu.append(score_dict['neu'])\n",
    "        compound.append(score_dict['compound'])\n",
    "        \n",
    "    #DataFrame is created using news id as primary key to map with our first news extracted dataframe df.\n",
    "    sentiment_frame=pd.DataFrame(list(zip(ids,pos,neu,neg,compound)),columns =['ids', 'pos','neu','neg','compound'])\n",
    "\n",
    "    final_frame=pd.merge(f, sentiment_frame,  left_index=True, right_index=True)  #merged on index\n",
    "    final_frame['compound']=final_frame['compound']*100.0\n",
    "    \n",
    "    #sentiment_key--> a nested funcion to entitle news with mood genre based on compound score.\n",
    "    def sentiment_key(x):\n",
    "        if (x>0 and x<50):\n",
    "            return \"Neutral\"\n",
    "        elif x<0:\n",
    "            return \"Negative\"\n",
    "        else:\n",
    "            return \"Positive\"\n",
    "    final_frame['Mood']=final_frame['compound'].apply(sentiment_key)\n",
    "    return final_frame   #table with news atttributes+mood genre\n",
    "    \n",
    "def csv_loader():\n",
    "    data=sentiment_analyzer()\n",
    "    ff=data.drop(columns=[\"pos\",\"neu\",\"neg\",\"ids\"])  #dropping unnecessary columns from final frame\n",
    "    print(\"Appending Top-5 News to CSV file...\")\n",
    "    \n",
    "    if os.path.exists(r\"C:\\Users\\cvb\\Documents\\automation_python\\Top-5 Headlines News\\News.csv\")==False:\n",
    "        ff.to_csv(\"News.csv\",mode='w', index=False, header=True)\n",
    "    else:\n",
    "        ff.to_csv(\"News.csv\",mode='a', index=False, header=False)\n",
    "    \n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n",
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n",
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n",
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n",
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n",
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n",
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n",
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n",
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n",
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n",
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n",
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n",
      "news_scraping Completed\n",
      "\n",
      "\n",
      "beautifulsoup_extract_element Completed\n",
      "\n",
      "\n",
      "sentiment_analyzer Completed\n",
      "\n",
      "\n",
      "Appending Top-5 News to CSV file...\n",
      "csv loader Completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-da6693a9b3c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# As the html page of top-5 news refreshes around in 30 min. So using schedule library to run the above functions as ETL job in every 30 min\n",
    "\n",
    "import time\n",
    "import schedule\n",
    "\n",
    "\n",
    "def job():\n",
    "    print(\"news_scraping Completed\\n\\n\")\n",
    "    time.sleep(2)\n",
    "    print(\"beautifulsoup_extract_element Completed\\n\\n\")\n",
    "    time.sleep(2)\n",
    "    print(\"sentiment_analyzer Completed\\n\\n\")\n",
    "    csv_loader()\n",
    "    print(\"csv loader Completed\")\n",
    "\n",
    "schedule.every(30).minutes.do(job)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
